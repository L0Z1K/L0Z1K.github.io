---
layout: post
title: NIPS 2021 Paper Review
category: research
---

1Day 1Paper 프로젝트의 첫 대상으로 NIPS 2021에 Accepted된 논문들을 봤다. 

[여기](https://papers.labml.ai/papers/neurips_2021)서 사람들이 Like한 순으로 논문을 정렬해놓아서 그 순서대로 하루에 하나씩 총 16일 동안, 14개의 논문을 살펴보았다. (왜 숫자가 다르지? ㅎㅎ..)

### Day 1. Pay attention to MLPs

[arxiv](https://arxiv.org/abs/2105.08050)

우리는 Attention mechanism이 제안된 이후로 무조건 Attention을 쓰는게 vision이든 nlp든 괜찮다라고 당연시하게 받아들였지만 굳이 Attention은 필요하지 않다고 저자들은 주장하였다. Attention 대신 gating function을 MLP에 추가한 gMLP를 사용하여 Attention을 사용한 것보다 더 좋거나 비슷한 성능을 여러 task에서 내었다. gating function에서 spatial information을 뽑을 수 있도록 한다.

### Day 2. MLP-Mixer: An all-MLP Architecture for Vision

[arxiv](https://arxiv.org/abs/2105.01601.pdf)

Day 1과 꽤 비슷한 이슈를 얘기한다. 아직 우리 MLP 죽지 않았다!라는 걸 얘기한다. ViT에서처럼 Image를 patch로 나누고 독립적으로 각각의 patch 안에서 MLP를 적용하고,(channel mixing) 각 patch의 같은 위치에 있는 애들끼리 MLP를 적용한다.(token mixing) token mixing을 통해 spatial information을 얻을 것을 기대한다. Day 1. 때보다 훨씬 구조도 간단해서 좋았고 JAX/Flax로 모델이 짜여졌는데 이걸 기반으로 JAX/Flax도 공부해보려고 한다! 재밌을 것 같다.

### Day 3. Do Vision Transformers See Like Convolutional Neural Networks?

[arxiv](https://arxiv.org/abs/2108.08810.pdf)

읽으면서 감탄했던 논문이다. 평소에 AI Explanability에는 별 관심이 없었다. 우리가 이해할 정도의 AI면 우리가 기대하는 것보다 더 좋은 성능이 나올 수 없다고 생각했기 때문이다. 이 논문은 어떻게 ViT가 CNN보다 좋은 성능을 내는지 설명한다. 두 모델이 차이가 있다는 것을 정량적으로 표현하는데 굉장히 재밌었다. 가장 인상깊었던 것은 이전에 ViT를 공부할 때 왜 ViT는 많은 데이터셋이 필요할까? 라고 생각했었는데 실제로 이 논문에서 ImageNet dataset으로만 학습한 ViT는 lower layer에서부터 너무 global한 정보를 보려고 하다보니 성능이 좋지 않다라고 얘기한다. CNN은 반면에 lower layer에서 강제적으로 local 정보를 보니 성능이 더 좋은 것이다. 뭔가 AI에서 논리적으로 질문이 해결되는게 굉장히 오랜만에 일이였어서 굉장히 인상깊었다!

### Day 4. Diffusion Models Beat GANs on Image Synthesis

[arxiv](https://arxiv.org/abs/2105.05233.pdf)

Diffusion model을 처음 접해서 제시간에 다 읽지는 못한 논문이다. 수식이 엄청 복잡해서 다 이해하는 건 어려웠다. Diffusion model의 개념을 접한 것에 의의를 두었다! 노이즈를 점점 주면서 그 끝에 Gaussian distribution의 노이즈로부터 역연산으로 Image를 생성한다는 것이 신박하긴 했다.

### Day 5. Distributed Deep Learning in Open Collaborations

[arxiv](https://arxiv.org/abs/2106.10207.pdf)

갈수록 모델의 크기가 커지면서 GPU 많은 기업과 가난한 개인이 모델에 접근할 수 있는 정도에 차이가 생기기 시작했다. 이런 간극을 줄이기 위해 pretrained된 모델의 checkpoint를 공유하는 수많은 model-hub이 나왔지만.. 그래도 쉽진 않다. 그래서 Collaborative Training이라는 것이 제안되었는데 여러 volunteer들이 gpu를 지원을 해주면 그 gpu를 모두 사용해서 training을 함으로써 더 빠른 학습을 기대한다. 하지만 지금까지의 Collaborative Training은 잘 일어나지 못했는데, 서버의 throughput이나 latency 이슈, volunteer들이 자기들 맘대로 지원을 했다가 안했을 때 생기는 연산의 consistency 이슈가 있기 때문이다. 그런데 이 논문에서 DeDLOC이라는 프레임워크?를 제안하면서 우리가 더 Collborative Training을 할 수 있는 환경을 만들었다.
이제 5일째 되어가는데 처음 보는 개념들도 접하고 많이 공부가 되는 것 같아서 뿌듯하다. Day 30까지 화이팅!

### Day 6. The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning

[arxiv](https://arxiv.org/abs/2109.02869.pdf)

내가 너무 모르는 분야 논문인거 같아서 이해를 거의 못했다. RL에서 multi-agent를 사용할 때 각 agent가 인지한 환경을 NN을 사용하여 represent하고 attention으로 서로 communicate해서 local한 정보들을 모아 global한 정보를 얻어낸다는 얘기다.

### Day 7. Multimodal Few-Shot Learning with Frozen Language Models

[arxiv](https://arxiv.org/abs/2106.13884.pdf)

엄청 큰 Language Model들이 Prompt learning을 통해서 굉장히 뛰어난 성능을 보여서 이를 이용해 Image랑 Text를 동시에 처리하는 Multimodal로도 확장한 논문이다. Image를 그저 Encoder를 통해 2개의 Token으로 만들어내서 그 후로는 Text token과 동일하게 Transformer가 처리하게 된다. 이때 Language model은 학습하지 않고(Frozen), Visual Encoder만 학습한다. 결과가 되게 신기하게 좋았다.

### Day 8. Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning

[arxiv](https://arxiv.org/abs/2106.02584.pdf)

non-parametric model 마냥 전체 데이터셋을 이용하여 Inference를 하게 되는데 이때 데이터 x들 간의 관계를 self-attention으로 구한다고 한다. 이 내용을 보고 바로 든 생각이 큰 데이터셋을 못 다룬다는 거였는데 이 논문에서도 그 해결법을 찾진 못하였다.
Computational Cost 때문에 Big Data에 대해서는 적용하기가 어려울 것 같은데 Big Data 시대인 지금에서 과연 novel한 논문인지는.. 잘 모르겠다.

### Day 10. Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions

[arxiv](https://arxiv.org/abs/2106.01798.pdf)

이 논문 솔직히 이해하기 너무 어렵다.. 한 3% 정도 이해한 것 같다. 우리는 보통 MLE 방법론을 사용해 최적의 모델을 찾아내는데 discret latent variable을 사용하게 되면 backpropagate되는 부분에서 gradient가 대부분이 0이 되서 update가 잘 이뤄나지 않는다. 그래서 Implicit MLE라는 방법론을 이 저자들이 제시하는데.. 그게 대체 뭘까?

### Day 11. DOBF: A Deobfuscation Pre-Training Objective for Programming Languages

[arxiv](https://arxiv.org/abs/2102.07492.pdf)

보통 NLP 모델을 Pretraining할 때는 Masked Language Modeling (MLM) 방법을 사용하는데, Code Generation Task에 있어서는 이 MLM이 너무 쉽다. 만약 변수명을 마스킹한다면, 그저 그 앞의 코드들을 보고 쉽게 변수명을 유추할 수 있다. 너무 쉽기 때문에 sub-optimal하다고 주장한다. 그러면서 DOBF라는 방법론을 제안한다. Code를 Obfuscate한 것을 Deobfuscate하는 식으로 모델을 학습하면 훨씬 Code에 대한 전반적인 이해를 할 수 있어서 성능이 좋다고 한다. 새로운 방법론인데 꽤 신박하고 그럴듯했다!

### Day 12. Deep Learning on a Data Diet: Finding Important Examples Early in Training

[arxiv](https://arxiv.org/abs/2107.07075.pdf)

이 논문도 되게 재밌었다. Cost를 줄이기 위해서 Data pruning을 하게 되는데 이 논문에서 그 기준을 Loss Gradient Norm이나 Error vector Norm으로 계산한다. 굉장히 그럴듯하다. 애초에 weight이 randomly initialization되어 있는 상태에서도 Loss 값이 작다(ground truth와 유사하게 예측한다)는 건 그 데이터는 학습할 가치가 비교적 떨어진다고 볼 수 있다. 하지만 이런 score을 하나의 weight에 대해서만 하면 오차가 생길 수 있어서 여러번 weight를 randomly initilization해주고 score를 구하여 평균을 낸다고 한다. 얘도 Flax/JAX로 구현되어 있네..

### Day 13. Decision Transformer: Reinforcement Learning via Sequence Modeling

[arxiv](https://arxiv.org/abs/2106.01345.pdf)

대Transformer시대에 걸맞는 논문이다. 이제 RL까지 Transformer로 먹으려 한다. 사실 이 논문이 나오기 전부터 Transformer를 RL에 적용하려는 시도는 많았다. 근데 다들 기존 RL 알고리즘을 Transformer로 대체하려는 시도고, 이 논문은 ViT에서 Image를 Transformer 입맛에 맞게 Sequence로 바꿔준 것처럼 RL Problem을 Sequnce modeling problem으로 바꿔버린다. 성능도 SOTA랑 비슷하거나 더 좋다고 한다.

### Day 14. Revisiting Deep Learning Models for Tabular Data

[arxiv](https://arxiv.org/abs/2106.11959.pdf)

시간 상 깊게 읽지는 못했다. DL Model이 워낙 좋으니까 Tabular Data에도 적용하려는 시도가 있는데 이쪽 연구가 워낙 기준이 안잡혀져 있어서 뭐가 best인지 비교하기가 어렵다고 한다. 그래서 이 논문에서 아예 그 benchmark를 잡아주고 FT-Transformer라는 tabular data용 Transformer를 개발해 좋은 성능을 보인다고 말한다. 근데 보통 tabular data는 데이터가 그렇게 많지 않은데 Transformer가 outperform이라는게 좀 신기했다..

### Day 16. CogView: Mastering Text-to-Image Generation via Transformers

[arxiv](https://arxiv.org/pdf/2105.13290.pdf)

DALL-E 다음에 나온 text-to-image generation model이다. 대놓고 DALL-E랑 비교하면서 우리가 더 낫다고 말한다. 성능도 좋고 학습에 있어서 안정성을 발전시켰다. PB-relaxation을 제안하여 LayerNorm 시 x를 max(x)로 나눠주고 attention에서는 K를 sqrt(d)로 먼저 나누고 Q랑 dot product하여 bottleneck을 감소시켰다. 또한 각 residual branch 앞뒷면에 Layer Normalization을 하는 Sandwich-LN도 도움이 된다고 한다.

- - -

### Conclusion

처음에 하루에 한 논문을 읽으면 깊게 읽기는 쉽지 않으니까 얻어가는게 적지 않을까? 라고 생각했는데 전혀 그렇지 않았다. 난 현재 우주의 먼지만큼만 알고있구나라는 걸 다시 깨달았고, 정말 공부할 것이 무궁무진하다는 것을 느꼈다. 계속해서 이렇게 정진하자.